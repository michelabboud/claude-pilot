---
description: "Spec implementation phase - TDD loop for each task in the plan"
argument-hint: "<path/to/plan.md>"
user-invocable: false
model: opus
---
# /spec-implement - Implementation Phase

**Phase 2 of the /spec workflow.** Reads the approved plan and implements each task using TDD (Red ‚Üí Green ‚Üí Refactor).

**Input:** Path to an approved plan file (`Approved: Yes`)
**Output:** All plan tasks completed, status set to COMPLETE
**Next phase:** On completion ‚Üí `Skill(skill='spec-verify', args='<plan-path>')`

---

## ‚õî KEY CONSTRAINTS (Rules Summary)

| # | Rule |
|---|------|
| 1 | **NO ad-hoc sub-agents** (exploration, research, general-purpose) - Use direct tools only. Exception: `pilot:spec-executor` agents for parallel wave execution (see Step 2.2b). |
| 2 | **TDD is MANDATORY** - No production code without failing test first |
| 3 | **Update plan checkboxes AND task status after EACH task** - Not at the end |
| 4 | **NEVER SKIP TASKS** - Every task MUST be fully implemented |
| 5 | **Quality over speed** - Never rush due to context pressure |
| 6 | **Plan file is source of truth** - Survives session clears |
| 7 | **NEVER assume - verify by reading files** |
| 8 | **Task management is MANDATORY** - Use TaskCreate/TaskUpdate for progress tracking |

---

## Quality Over Speed - CRITICAL

**NEVER rush or compromise quality due to context pressure.**

- Context warnings are informational, not emergencies
- Work spans sessions seamlessly via plan file and continuation mechanisms
- Finish the CURRENT task with full quality, then hand off cleanly
- Do NOT skip tests, compress code, or cut corners to "beat" context limits
- **Quality is the #1 metric** - a well-done task split across sessions beats rushed work

## Feedback Loop Awareness

**This phase may be called multiple times in a feedback loop:**

```
spec-implement ‚Üí spec-verify ‚Üí issues found ‚Üí spec-implement ‚Üí spec-verify ‚Üí ... ‚Üí VERIFIED
```

**When called after verification found issues:**
1. Read the plan - verification will have added fix tasks (marked with `[MISSING]` or similar)
2. Check the `Iterations` field in the plan header
3. **Report iteration start:** "üîÑ Starting Iteration N implementation..."
4. Focus on uncompleted tasks `[ ]` - these are the fixes needed
5. Complete all fix tasks, then set status to COMPLETE as normal

---

### Step 2.1: Read Plan & Gather Context

**Before ANY implementation, you MUST:**

1. **Read the COMPLETE plan** - Understanding overall architecture and design
2. **Verify comprehension** - Summarize what you learned to demonstrate understanding
3. **Identify dependencies** - List files, functions, classes that need modification
4. **Check current state:**
   - Git status: `git status --short` and `git diff --name-only`
   - Plan progress: Check for `[x]` completed tasks

#### üîß Tools for Implementation

| Tool | When to Use | Example |
|------|-------------|---------|
| **Context7** | Library API lookup | `resolve-library-id(query="how to use fixtures", libraryName="pytest")` then `query-docs(libraryId, query)` |
| **Vexor** | Find similar patterns | `vexor search "query" --mode code` |
| **grep-mcp** | Production code examples | `searchGitHub(query="useEffect cleanup", language=["TypeScript"])` |

---

### Step 2.1b: Create or Resume Worktree (Conditional)

**Check the plan's `Worktree:` header field to determine isolation mode.**

0. **Read the `Worktree:` header from the plan file:**
   - Parse `Worktree: Yes` or `Worktree: No` from the plan content (regex: `/^Worktree:\s*(\w+)/m`)
   - If the field is missing, default to `Yes` (backward compatibility with older plans)
   - **If `Worktree: No`:** Skip the rest of Step 2.1b entirely. Implementation happens directly on the current branch. Proceed to Step 2.2.

**If `Worktree: Yes` (or missing/default):** All implementation happens in an isolated git worktree. This keeps the main branch clean until verification passes and the user approves sync.

1. **Extract plan slug** from the plan file path:
   - `docs/plans/2026-02-09-add-auth.md` ‚Üí plan_slug = `add-auth` (strip date prefix and `.md`)

2. **Check for existing worktree** (continuation session or verify‚Üíimplement feedback loop):
   ```bash
   # Use the worktree module to detect an existing worktree
   uv run python -c "
   from launcher.worktree import detect_worktree
   from pathlib import Path
   info = detect_worktree(Path('<project_root>'), '<plan_slug>')
   if info: print(f'FOUND:{info.path}:{info.branch}:{info.base_branch}')
   else: print('NONE')
   "
   ```

3. **If worktree exists:** Resume it ‚Äî set CWD to the worktree path via wrapper pipe command:
   ```bash
   ~/.pilot/bin/pilot pipe set-worktree <worktree_path>
   ```
   Then `cd` to the worktree path for all subsequent commands.

4. **If no worktree exists:** Create one:
   ```bash
   uv run python -c "
   from launcher.worktree import create_worktree
   from pathlib import Path
   info = create_worktree(Path('<project_root>'), '<plan_slug>')
   print(f'CREATED:{info.path}:{info.branch}:{info.base_branch}')
   "
   ```
   Then set CWD via wrapper pipe command and `cd` to the worktree path.

5. **If creation fails due to dirty working tree:** Report to user and ask them to stash or commit changes first. Do NOT proceed with implementation until the worktree is created.

6. **If creation fails due to old git version** (error contains "git >= 2.15 required"): Log a warning and continue without worktree isolation. Implementation will happen directly on the current branch. This is a graceful fallback for systems with older git versions.

7. **Verify worktree is active:** Run `git branch --show-current` in the worktree to confirm you're on the `spec/<plan_slug>` branch.

**‚ö†Ô∏è All subsequent implementation steps happen inside the worktree directory (when worktree is active).** The plan file exists at the same relative path in the worktree (e.g., `docs/plans/...`). Commits within the worktree are expected and allowed.

---

### Step 2.2: Set Up Task List (MANDATORY)

**After reading the plan, set up task tracking using the Task management tools.**

This makes implementation progress visible in the terminal (Ctrl+T), enables dependency tracking, and persists across session handoffs via `CLAUDE_CODE_TASK_LIST_ID`.

**Process:**

1. **Check for existing tasks first:** Run `TaskList` to see if a previous session already created tasks
2. **Branch based on result:**

**If TaskList returns tasks (continuation session):**
- Tasks already exist from a prior session - do NOT recreate them
- Review existing task statuses to understand where the previous session left off
- Cross-reference with plan checkboxes (`[x]` = done, `[ ]` = remaining)
- If a task is `in_progress` but the session that started it is gone, it was interrupted - keep it `in_progress` and resume it
- Proceed to Step 2.3 starting with the first uncompleted task

**If TaskList is empty (fresh start):**
- Create one task per uncompleted plan task (`[ ]` items):
  ```
  TaskCreate(
    subject="Task N: <title from plan>",
    description="<objective + implementation steps from plan>",
    activeForm="Implementing <short description>"
  )
  ```
- Set up dependencies if tasks have ordering requirements:
  ```
  TaskUpdate(taskId="<task3_id>", addBlockedBy=["<task2_id>"])
  ```
- Skip already-completed plan tasks (`[x]` items) - don't create tasks for them

**Example for a fresh start with 4 tasks:**
```
TaskCreate: "Task 1: Create user model"           ‚Üí id=1
TaskCreate: "Task 2: Add API endpoints"            ‚Üí id=2, addBlockedBy: [1]
TaskCreate: "Task 3: Write integration tests"      ‚Üí id=3, addBlockedBy: [2]
TaskCreate: "Task 4: Add documentation"            ‚Üí id=4, addBlockedBy: [2]
```

**Why this matters:**
- User sees real-time progress in their terminal via status spinners
- Dependencies prevent skipping ahead when tasks have ordering requirements
- Tasks persist across session handoffs (stored in `~/.claude/tasks/`)
- Continuation sessions pick up exactly where the previous session left off

---

### Step 2.2b: Wave Detection and Parallel Execution (Optional)

**After setting up the task list, detect if independent tasks can run in parallel.**

Wave-based execution spawns `pilot:spec-executor` subagents for independent tasks, each with a fresh context window. This is inspired by GSD's parallel plan execution model, adapted to Pilot's single-plan architecture.

#### When to Use Parallel Waves

| Condition | Execution Mode |
|-----------|---------------|
| Plan has `Wave:` markers on tasks | Follow wave grouping from plan |
| Tasks have `Dependencies:` fields | Auto-detect waves from dependency graph |
| Tasks share files in "Modify" lists | Run those tasks **sequentially** (conflict risk) |
| Only 1 task remaining | Run directly (no parallelism benefit) |
| All tasks depend on each other | Run sequentially as normal (Step 2.3) |

#### Wave Detection Algorithm

```
1. Parse each uncompleted task's Dependencies and Modify file lists
2. Build dependency graph:
   - Task B depends on Task A ‚Üí B must wait for A
   - Task B and C modify the same file ‚Üí B must wait for C (or vice versa)
3. Group into waves:
   - Wave 1: Tasks with no dependencies and no file conflicts
   - Wave 2: Tasks that depend only on Wave 1 tasks
   - Wave N: Tasks that depend only on Wave 1..N-1 tasks
4. If a wave has only 1 task ‚Üí run directly (no subagent overhead)
5. If a wave has 2+ tasks ‚Üí spawn parallel spec-executor subagents
```

#### Parallel Execution Protocol

**For each wave with 2+ independent tasks:**

1. **Prepare task context** for each executor:
   ```
   Task number, full task definition (objective, files, key decisions, DoD),
   plan summary (goal, tech stack, scope), project root path
   ```

2. **Spawn parallel executors** using a single message with multiple Task tool calls:
   ```
   Task(
     subagent_type="pilot:spec-executor",
     prompt="Execute Task N from the plan...\n\nTask Definition:\n{task_def}\n\nPlan Context:\n{plan_summary}\n\nProject Root: {project_root}",
     description="Spec executor: Task N"
   )
   ```
   **Send ALL executor calls in ONE message** for true parallelism.

3. **Collect results** from each executor:
   - Parse the JSON output for status, files_changed, dod_checklist
   - Verify each task's DoD criteria are met
   - If any executor reports `failed` or `blocked`, handle before proceeding

4. **After all executors complete:**
   - Run the full test suite to check for cross-task conflicts
   - Update plan checkboxes for all completed tasks (Step 2.4)
   - Mark completed tasks in the task list
   - Proceed to next wave

#### Sequential Fallback

**If wave detection finds all tasks are dependent (no parallelism possible), skip this step entirely and proceed to Step 2.3 (sequential TDD loop).** This is the default behavior ‚Äî parallel waves are an optimization, not a requirement.

#### Error Handling

| Situation | Action |
|-----------|--------|
| Executor returns `failed` | Read the failure reason, fix directly (don't re-spawn), continue |
| Executor returns `blocked` | Check if blocker is another task, reorder if needed |
| Cross-task test failures after wave | Fix conflicts directly before next wave |
| Executor times out | Run task directly in main context |

---

### Step 2.3: Per-Task TDD Loop

**TDD is MANDATORY. No production code without a failing test first.**

| Requires TDD | Skip TDD |
|--------------|----------|
| New functions/methods | Documentation changes |
| API endpoints | Config file updates |
| Business logic | IaC code (CDK, Terraform, Pulumi) |
| Bug fixes | Formatting/style changes |

**For EVERY task, follow this exact sequence:**

1. **READ PLAN'S IMPLEMENTATION STEPS** - List all files to create/modify/delete
2. **Perform Call Chain Analysis:**
   - **Trace Upwards (Callers):** Identify what calls the code you're modifying
   - **Trace Downwards (Callees):** Identify what the modified code calls
   - **Side Effects:** Check for database, cache, external system impacts
3. **Mark task as `in_progress`** - `TaskUpdate(taskId="<id>", status="in_progress")`
4. **Execute TDD Flow (RED ‚Üí GREEN ‚Üí REFACTOR):**
   - Write failing test first, **verify it fails**
   - Implement minimal code to pass
   - Refactor if needed (keep tests green)
5. **Verify tests pass** - Run the project's test runner (e.g., `uv run pytest -q`, `bun test`, `npm test`)
6. **Run actual program** - Use the plan's Runtime Environment section to start the service/program. Show real output with sample data.
7. **Check diagnostics** - Must be zero errors
8. **Validate Definition of Done** - Check all criteria from plan
9. **Per-task commit (worktree mode only)** - If `Worktree: Yes` in the plan, commit task changes immediately:
   ```bash
   git add <task-specific-files>  # Stage only files related to this task
   git commit -m "{type}(spec): {task-name}"
   ```
   Use `feat(spec):` for new features, `fix(spec):` for bug fixes, `test(spec):` for test-only tasks, `refactor(spec):` for refactoring. Skip this step when `Worktree: No` (normal git rules apply).
10. **Mark task as `completed`** - `TaskUpdate(taskId="<id>", status="completed")`
11. **UPDATE PLAN FILE IMMEDIATELY** (see Step 2.4)
12. **Check context usage** - Run `~/.pilot/bin/pilot check-context --json`

**‚ö†Ô∏è NEVER SKIP TASKS:**
- EVERY task MUST be fully implemented
- NO exceptions for "MVP scope" or complexity
- If blocked: STOP and report specific blockers
- NEVER mark complete without doing the work

---

### Step 2.4: Update Plan After EACH Task

**‚õî CRITICAL: Task Completion Tracking is MANDATORY**

**After completing EACH task, you MUST:**

1. **IMMEDIATELY edit the plan file** to change `[ ]` to `[x]` for that task
2. **Update the Progress Tracking counts** (Completed/Remaining)
3. **DO NOT proceed to next task** until the checkbox is updated

**This is NON-NEGOTIABLE.**

**Example - After completing Task 5:**
```
Edit the plan file:
- [ ] Task 5: Implement X  ‚Üí  - [x] Task 5: Implement X
Update counts:
**Completed:** 4 | **Remaining:** 8  ‚Üí  **Completed:** 5 | **Remaining:** 7
```

---

### Step 2.5: All Tasks Complete ‚Üí Verification

**‚ö†Ô∏è CRITICAL: Follow these steps exactly:**

1. Quick verification: Check diagnostics and run the project's test suite
2. **FOR MIGRATIONS ONLY - Feature Parity Check:**
   - Run the NEW code and verify it produces expected output
   - Compare behavior with OLD code (if still available)
   - Check Feature Inventory - every feature should now be implemented
   - If ANY feature is missing: **DO NOT mark complete** - add tasks for missing features
3. **MANDATORY: Update plan status to COMPLETE**
   ```
   Edit the plan file and change the Status line:
   Status: PENDING  ‚Üí  Status: COMPLETE
   ```
4. **Register status change:** `~/.pilot/bin/pilot register-plan "<plan_path>" "COMPLETE" 2>/dev/null || true`
5. **‚õî Phase Transition Context Guard:** Run `~/.pilot/bin/pilot check-context --json`. If >= 80%, hand off instead (see spec.md Section 0.3).
6. **Invoke verification phase:** `Skill(skill='spec-verify', args='<plan-path>')`

---

## ‚ö†Ô∏è Migration/Refactoring Tasks (Phase 2 Additions)

**When the plan involves replacing existing code, perform these ADDITIONAL checks:**

### Before Starting Implementation

1. **Locate the Feature Inventory section** in the plan
2. **If Feature Inventory is MISSING** - STOP and inform user
3. **Verify ALL features are mapped** - Every row must have a Task #
4. **Read the OLD code completely** - Don't rely on the plan alone

### During Implementation

For EACH task that migrates old functionality:

1. **Read the corresponding old file(s)** listed in Feature Inventory
2. **Create a checklist** of functions/behaviors from old code
3. **Verify each function/behavior exists** in new code after implementation
4. **Test with same inputs** - Old and new code should produce same outputs

### Before Marking Task Complete

**For migration tasks, add this to Definition of Done:**

- [ ] All functions from old code have equivalents in new code
- [ ] Behavior matches old code (same inputs ‚Üí same outputs)
- [ ] No features accidentally omitted

### Red Flags - STOP Implementation

If you notice ANY of these, STOP and report to user:

- Feature Inventory section missing from plan
- Old file has functions not mentioned in any task
- "Out of Scope" items that should actually be migrated
- Tests pass but functionality is missing compared to old code

---

## Context Management (90% Handoff)

After each major operation, check context:

```bash
~/.pilot/bin/pilot check-context --json
```

**Between iterations:**
1. If context >= 90%: hand off cleanly (don't rush!)
2. If context 80-89%: continue but wrap up current task with quality
3. If context < 80%: continue the loop freely

If response shows `"status": "CLEAR_NEEDED"` (context >= 90%):

**‚ö†Ô∏è CRITICAL: Execute ALL steps below in a SINGLE turn. DO NOT stop or wait for user response between steps.**

**Step 1: Write continuation file (GUARANTEED BACKUP)**

Write to `~/.pilot/sessions/$PILOT_SESSION_ID/continuation.md`:

```markdown
# Session Continuation (/spec)

**Plan:** <plan-path>
**Phase:** implementation
**Current Task:** Task N - [description]

**Completed This Session:**
- [x] [What was finished]

**Next Steps:**
1. [What to do immediately when resuming]

**Context:**
- [Key decisions or blockers]
```

**Step 2: Trigger session clear**

```bash
~/.pilot/bin/pilot send-clear <plan-path>
```

Pilot will restart with `/spec --continue <plan-path>`

ARGUMENTS: $ARGUMENTS
